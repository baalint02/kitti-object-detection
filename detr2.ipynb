{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kitti_detection import config\n",
    "from kitti_detection.dataset import DataSample, class_names, load_train_val_test_dataset\n",
    "from kitti_detection.utils import display_samples_h\n",
    "from kitti_detection import box_utils\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.tv_tensors import BoundingBoxes\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH_FACTOR, HEIGHT_FACTOR = 1/370, 1/370"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeBoundingBoxes(v2.Transform):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        target['boxes'] *= torch.tensor([WIDTH_FACTOR, HEIGHT_FACTOR, WIDTH_FACTOR, HEIGHT_FACTOR])\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = v2.Compose([\n",
    "    v2.RandomCrop(size=(370, 370)),\n",
    "    v2.SanitizeBoundingBoxes(),\n",
    "    v2.ToDtype(torch.float32),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    v2.ConvertBoundingBoxFormat(format='cxcywh'),\n",
    "    NormalizeBoundingBoxes(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = class_names[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(t):\n",
    "    print(t)\n",
    "    print(t.size())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1d_pos_encoding(l, dim):\n",
    "    return torch.cat([\n",
    "        torch.stack([\n",
    "            torch.linspace(0, 10000**(2*i/dim), steps=l).sin(),\n",
    "            torch.linspace(0, 10000**(2*i/dim), steps=l).cos()\n",
    "        ], dim=1)\n",
    "        for i in range(dim // 2)\n",
    "    ], dim=1)\n",
    "\n",
    "def create_pos_encoding(h, w, dim):\n",
    "    col_embed = get_1d_pos_encoding(w, dim // 2).repeat(h, 1, 1)\n",
    "    row_embed = get_1d_pos_encoding(h, dim // 2).unsqueeze(1).repeat(1, w, 1)\n",
    "    \n",
    "    return torch.cat((col_embed, row_embed), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DETR(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_embed=256):\n",
    "        super().__init__()\n",
    "        self.backbone = self._backbone()\n",
    "        self.conv = nn.Conv2d(512, dim_embed, kernel_size=1)\n",
    "\n",
    "        self.register_buffer('pos_embedding', create_pos_encoding(12, 12, dim_embed)) # (12, 12, 256)\n",
    "        self.register_buffer('query_pos_embedding', get_1d_pos_encoding(20, dim_embed))\n",
    "\n",
    "        self.transformer = nn.Transformer(dim_embed, nhead=8, num_encoder_layers=4, num_decoder_layers=4, batch_first=True)\n",
    "\n",
    "        self.linear_class = nn.Linear(dim_embed, n_classes + 1)\n",
    "        self.linear_bbox = nn.Linear(dim_embed, 4)\n",
    "\n",
    "    def _backbone(self) -> nn.Module:\n",
    "        backbone = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        del backbone.fc\n",
    "        del backbone.avgpool\n",
    "\n",
    "        def _forward(bb: torchvision.models.ResNet, x):\n",
    "            x = bb.conv1(x)\n",
    "            x = bb.bn1(x)\n",
    "            x = bb.relu(x)\n",
    "            x = bb.maxpool(x)\n",
    "\n",
    "            x = bb.layer1(x)\n",
    "            x = bb.layer2(x)\n",
    "            x = bb.layer3(x)\n",
    "            x = bb.layer4(x)\n",
    "            return x\n",
    "\n",
    "        backbone.forward = lambda x: _forward(backbone, x)\n",
    "        # TODO BatchNorm freeze\n",
    "        return backbone\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size = input.shape[0]\n",
    "\n",
    "        x = self.backbone(input) # (4, 512, 12, 12)\n",
    "        x = self.conv(x) # (4, 256, 12, 12)\n",
    "\n",
    "        x = x.permute(0, 2, 3, 1) # (4, 12, 12, 256)\n",
    "        x = x + self.pos_embedding\n",
    "        x = x.flatten(1, 2) # (4, 144, 256)\n",
    "\n",
    "        q = self.query_pos_embedding.repeat(batch_size, 1, 1) # (4, 20, 256)\n",
    "        q = self.transformer(x, q)\n",
    "\n",
    "        return {\n",
    "            'pred_logits': self.linear_class(q),\n",
    "            'pred_boxes': torch.sigmoid(self.linear_bbox(q))\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _collate(samples):\n",
    "    imgs = tuple( img for img, _ in samples )\n",
    "    targets = tuple( target for _, target in samples )\n",
    "    return torch.stack(imgs), tuple(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def match_indices(outputs: torch.Tensor, targets: torch.Tensor, cost_bbox_factor=0.3, cost_class_factor=0.3, cost_giou_factor=0.3):\n",
    "    \"\"\" Performs the matching\n",
    "\n",
    "    Params:\n",
    "        outputs: This is a dict that contains at least these entries:\n",
    "                \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n",
    "                \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n",
    "\n",
    "        targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n",
    "                \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth\n",
    "                        objects in the target) containing the class labels\n",
    "                \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n",
    "\n",
    "    Returns:\n",
    "        A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
    "            - index_i is the indices of the selected predictions (in order)\n",
    "            - index_j is the indices of the corresponding selected targets (in order)\n",
    "        For each batch element, it holds:\n",
    "            len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n",
    "    \"\"\"\n",
    "    bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
    "\n",
    "    # We flatten to compute the cost matrices in a batch\n",
    "    out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
    "    out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
    "\n",
    "    # Also concat the target labels and boxes\n",
    "    tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
    "    tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])\n",
    "\n",
    "    # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
    "    # but approximate it in 1 - proba[target class].\n",
    "    # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
    "    cost_class = -out_prob[:, tgt_ids]\n",
    "\n",
    "    # Compute the L1 cost between boxes\n",
    "    cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
    "\n",
    "    # Compute the giou cost betwen boxes\n",
    "    cost_giou = -box_utils.generalized_box_iou(box_utils.box_cxcywh_to_xyxy(out_bbox), box_utils.box_cxcywh_to_xyxy(tgt_bbox))\n",
    "\n",
    "    # Final cost matrix\n",
    "    C = cost_bbox_factor * cost_bbox + cost_class_factor * cost_class + cost_giou_factor * cost_giou\n",
    "    C = C.view(bs, num_queries, -1).cpu()\n",
    "\n",
    "    sizes = [len(v[\"boxes\"]) for v in targets]\n",
    "    indices = [linear_sum_assignment(c[i].detach()) for i, c in enumerate(C.split(sizes, -1))]\n",
    "    return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HungarianLoss(nn.Module):\n",
    "    \"\"\" This class computes the loss for DETR.\n",
    "    The process happens in two steps:\n",
    "        1) we compute hungarian assignment between ground truth boxes and the outputs of the model\n",
    "        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, class_weights, empty_weight, loss_weight_dict):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        class_weights = torch.tensor(class_weights)\n",
    "        class_weights = torch.cat([class_weights, torch.tensor([empty_weight])])\n",
    "        self.loss_weight_dict = loss_weight_dict\n",
    "        self.register_buffer('class_weights', class_weights)\n",
    "\n",
    "    def get_loss_labels(self, outputs, targets, indices):\n",
    "        \"\"\"Classification loss (NLL)\n",
    "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n",
    "        \"\"\"\n",
    "        src_logits = outputs['pred_logits']\n",
    "\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
    "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
    "                                    dtype=torch.int64, device=src_logits.device)\n",
    "        target_classes[idx] = target_classes_o\n",
    "\n",
    "        loss = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.class_weights.to(src_logits.device))\n",
    "        return loss\n",
    "\n",
    "    def get_loss_boxes(self, outputs, targets, indices, num_boxes):\n",
    "        \"\"\"Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss\n",
    "           targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]\n",
    "           The target boxes are expected in format (center_x, center_y, w, h), normalized by the image size.\n",
    "        \"\"\"\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        src_boxes = outputs['pred_boxes'][idx]\n",
    "        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
    "\n",
    "        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')\n",
    "        loss_bbox = loss_bbox.sum() / num_boxes\n",
    "\n",
    "        loss_giou = 1 - torch.diag(box_utils.generalized_box_iou(\n",
    "            box_utils.box_cxcywh_to_xyxy(src_boxes),\n",
    "            box_utils.box_cxcywh_to_xyxy(target_boxes)))\n",
    "        loss_giou = loss_giou.sum() / num_boxes\n",
    "        \n",
    "        return loss_bbox, loss_giou\n",
    "\n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "        # permute predictions following indices\n",
    "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
    "        src_idx = torch.cat([src for (src, _) in indices])\n",
    "        return batch_idx, src_idx\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        indices = match_indices(outputs, targets)\n",
    "\n",
    "        num_boxes = sum(len(t[\"labels\"]) for t in targets)\n",
    "        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=outputs['pred_logits'].device)\n",
    "\n",
    "        loss_labels = self.get_loss_labels(outputs, targets, indices)\n",
    "        if num_boxes > 0:\n",
    "            loss_bbox, loss_giou = self.get_loss_boxes(outputs, targets, indices, num_boxes)\n",
    "        else:\n",
    "            loss_bbox = loss_giou = 0\n",
    "\n",
    "        losses = {\n",
    "            'loss_labels': loss_labels,\n",
    "            'loss_bbox': loss_bbox,\n",
    "            'loss_giou': loss_giou,\n",
    "        }\n",
    "\n",
    "        loss_sum = sum([ weight * losses[key] for key, weight in self.loss_weight_dict.items() ])\n",
    "        return loss_sum, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, test_dataset = load_train_val_test_dataset()\n",
    "\n",
    "train_dataset.transform = transforms\n",
    "valid_dataset.transform = transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=_collate)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DETR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def targets_to_device(targets, device):\n",
    "    return tuple({k: v.to(device) for k, v in target.items()} for target in targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module,\n",
    "                device: str,\n",
    "                train_loader: DataLoader,\n",
    "                valid_loader: DataLoader,\n",
    "                batch_size: int,\n",
    "                loss_fn: nn.Module,\n",
    "                optimizer: optim.Optimizer,\n",
    "                epochs: int,\n",
    "                scheduler: optim.lr_scheduler.LRScheduler = None,\n",
    "                early_stopping_patience: int = None\n",
    "                ):\n",
    "\n",
    "    model = model.to(device)\n",
    "    num_train_samples = len(train_loader.dataset)\n",
    "    best_loss = float('inf')\n",
    "    patience = early_stopping_patience\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # train\n",
    "        num_batches = len(train_loader)\n",
    "        model = model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch, (X, y) in enumerate(train_loader):\n",
    "            #print(f'[{epoch + 1:>2}/{epochs}]:', end='\\r')\n",
    "            X = X.to(device)\n",
    "            y = targets_to_device(y, device)\n",
    "            \n",
    "            pred = model(X)\n",
    "            loss, loss_dict = loss_fn(pred, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            train_loss += loss\n",
    "            print(f'[{epoch + 1:>2}/{epochs}]: train_loss={loss:>5f}  [{current:>5d}/{num_train_samples:>5d}]')\n",
    "        train_loss /= num_batches\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # validate\n",
    "        model = model.eval()\n",
    "        n_samples = 0\n",
    "        num_batches = len(valid_loader)\n",
    "        val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in valid_loader:\n",
    "                X = X.to(device)\n",
    "                y = targets_to_device(y, device)\n",
    "                pred = model(X)\n",
    "                loss, _  = loss_fn(pred, y)\n",
    "                val_loss += loss.item()\n",
    "                n_samples += len(X)\n",
    "\n",
    "        val_loss /= num_batches\n",
    "        print(f'[{epoch + 1:>2}/{epochs}]: {train_loss=:5f} {val_loss=:5.5f}')\n",
    "        \n",
    "        patience -= 1\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), config.CURRENT_MODEL_PATH)\n",
    "            patience = early_stopping_patience\n",
    "        if patience == 0:\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(torch.load(config.CURRENT_MODEL_PATH))\n",
    "    print(f'\\nbest model:')\n",
    "    print(f'\\tval_loss={best_loss:2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/2]: train_loss=1.467735  [   16/ 5237]\n",
      "[ 1/2]: train_loss=1.642001  [   32/ 5237]\n",
      "[ 1/2]: train_loss=1.448709  [   48/ 5237]\n",
      "[ 1/2]: train_loss=1.375795  [   64/ 5237]\n",
      "[ 1/2]: train_loss=1.402847  [   80/ 5237]\n",
      "[ 1/2]: train_loss=1.237998  [   96/ 5237]\n",
      "[ 1/2]: train_loss=0.971650  [  112/ 5237]\n",
      "[ 1/2]: train_loss=1.066948  [  128/ 5237]\n",
      "[ 1/2]: train_loss=1.060470  [  144/ 5237]\n",
      "[ 1/2]: train_loss=0.921089  [  160/ 5237]\n",
      "[ 1/2]: train_loss=0.946602  [  176/ 5237]\n",
      "[ 1/2]: train_loss=0.905962  [  192/ 5237]\n",
      "[ 1/2]: train_loss=1.085009  [  208/ 5237]\n",
      "[ 1/2]: train_loss=0.930899  [  224/ 5237]\n",
      "[ 1/2]: train_loss=0.979612  [  240/ 5237]\n",
      "[ 1/2]: train_loss=1.068821  [  256/ 5237]\n",
      "[ 1/2]: train_loss=0.973756  [  272/ 5237]\n",
      "[ 1/2]: train_loss=0.992349  [  288/ 5237]\n",
      "[ 1/2]: train_loss=0.993598  [  304/ 5237]\n",
      "[ 1/2]: train_loss=1.083637  [  320/ 5237]\n",
      "[ 1/2]: train_loss=1.039406  [  336/ 5237]\n",
      "[ 1/2]: train_loss=0.912185  [  352/ 5237]\n",
      "[ 1/2]: train_loss=0.912870  [  368/ 5237]\n",
      "[ 1/2]: train_loss=1.046985  [  384/ 5237]\n",
      "[ 1/2]: train_loss=0.907928  [  400/ 5237]\n",
      "[ 1/2]: train_loss=1.031794  [  416/ 5237]\n",
      "[ 1/2]: train_loss=0.932226  [  432/ 5237]\n",
      "[ 1/2]: train_loss=0.959629  [  448/ 5237]\n",
      "[ 1/2]: train_loss=0.968433  [  464/ 5237]\n",
      "[ 1/2]: train_loss=0.959057  [  480/ 5237]\n",
      "[ 1/2]: train_loss=0.856171  [  496/ 5237]\n",
      "[ 1/2]: train_loss=0.958196  [  512/ 5237]\n",
      "[ 1/2]: train_loss=1.056872  [  528/ 5237]\n",
      "[ 1/2]: train_loss=1.041198  [  544/ 5237]\n",
      "[ 1/2]: train_loss=1.050355  [  560/ 5237]\n",
      "[ 1/2]: train_loss=0.927107  [  576/ 5237]\n",
      "[ 1/2]: train_loss=0.983750  [  592/ 5237]\n",
      "[ 1/2]: train_loss=0.950660  [  608/ 5237]\n",
      "[ 1/2]: train_loss=1.000294  [  624/ 5237]\n",
      "[ 1/2]: train_loss=1.067681  [  640/ 5237]\n",
      "[ 1/2]: train_loss=0.965938  [  656/ 5237]\n",
      "[ 1/2]: train_loss=1.124167  [  672/ 5237]\n",
      "[ 1/2]: train_loss=1.103991  [  688/ 5237]\n",
      "[ 1/2]: train_loss=0.808041  [  704/ 5237]\n",
      "[ 1/2]: train_loss=0.916747  [  720/ 5237]\n",
      "[ 1/2]: train_loss=0.816457  [  736/ 5237]\n",
      "[ 1/2]: train_loss=0.840564  [  752/ 5237]\n",
      "[ 1/2]: train_loss=0.936510  [  768/ 5237]\n",
      "[ 1/2]: train_loss=1.178460  [  784/ 5237]\n",
      "[ 1/2]: train_loss=0.907590  [  800/ 5237]\n",
      "[ 1/2]: train_loss=0.907392  [  816/ 5237]\n",
      "[ 1/2]: train_loss=0.896145  [  832/ 5237]\n",
      "[ 1/2]: train_loss=0.923593  [  848/ 5237]\n",
      "[ 1/2]: train_loss=0.872309  [  864/ 5237]\n",
      "[ 1/2]: train_loss=0.806066  [  880/ 5237]\n",
      "[ 1/2]: train_loss=0.927088  [  896/ 5237]\n",
      "[ 1/2]: train_loss=0.909978  [  912/ 5237]\n",
      "[ 1/2]: train_loss=0.839964  [  928/ 5237]\n",
      "[ 1/2]: train_loss=1.015065  [  944/ 5237]\n",
      "[ 1/2]: train_loss=0.933232  [  960/ 5237]\n",
      "[ 1/2]: train_loss=0.842695  [  976/ 5237]\n",
      "[ 1/2]: train_loss=0.968258  [  992/ 5237]\n",
      "[ 1/2]: train_loss=0.975412  [ 1008/ 5237]\n",
      "[ 1/2]: train_loss=0.895774  [ 1024/ 5237]\n",
      "[ 1/2]: train_loss=0.917419  [ 1040/ 5237]\n",
      "[ 1/2]: train_loss=0.827805  [ 1056/ 5237]\n",
      "[ 1/2]: train_loss=0.888878  [ 1072/ 5237]\n",
      "[ 1/2]: train_loss=0.740090  [ 1088/ 5237]\n",
      "[ 1/2]: train_loss=0.833638  [ 1104/ 5237]\n",
      "[ 1/2]: train_loss=0.845173  [ 1120/ 5237]\n",
      "[ 1/2]: train_loss=0.859723  [ 1136/ 5237]\n",
      "[ 1/2]: train_loss=0.866407  [ 1152/ 5237]\n",
      "[ 1/2]: train_loss=0.868174  [ 1168/ 5237]\n",
      "[ 1/2]: train_loss=0.773532  [ 1184/ 5237]\n",
      "[ 1/2]: train_loss=0.820431  [ 1200/ 5237]\n",
      "[ 1/2]: train_loss=0.723607  [ 1216/ 5237]\n",
      "[ 1/2]: train_loss=0.786360  [ 1232/ 5237]\n",
      "[ 1/2]: train_loss=0.949482  [ 1248/ 5237]\n",
      "[ 1/2]: train_loss=0.728082  [ 1264/ 5237]\n",
      "[ 1/2]: train_loss=0.827601  [ 1280/ 5237]\n",
      "[ 1/2]: train_loss=0.918902  [ 1296/ 5237]\n",
      "[ 1/2]: train_loss=0.771898  [ 1312/ 5237]\n",
      "[ 1/2]: train_loss=0.856457  [ 1328/ 5237]\n",
      "[ 1/2]: train_loss=0.774681  [ 1344/ 5237]\n",
      "[ 1/2]: train_loss=0.753263  [ 1360/ 5237]\n",
      "[ 1/2]: train_loss=0.844072  [ 1376/ 5237]\n",
      "[ 1/2]: train_loss=0.726080  [ 1392/ 5237]\n",
      "[ 1/2]: train_loss=0.809603  [ 1408/ 5237]\n",
      "[ 1/2]: train_loss=0.971965  [ 1424/ 5237]\n",
      "[ 1/2]: train_loss=0.881022  [ 1440/ 5237]\n",
      "[ 1/2]: train_loss=0.861584  [ 1456/ 5237]\n",
      "[ 1/2]: train_loss=0.888881  [ 1472/ 5237]\n",
      "[ 1/2]: train_loss=0.818022  [ 1488/ 5237]\n",
      "[ 1/2]: train_loss=0.737847  [ 1504/ 5237]\n",
      "[ 1/2]: train_loss=0.788080  [ 1520/ 5237]\n",
      "[ 1/2]: train_loss=0.911994  [ 1536/ 5237]\n",
      "[ 1/2]: train_loss=0.792655  [ 1552/ 5237]\n",
      "[ 1/2]: train_loss=0.855674  [ 1568/ 5237]\n",
      "[ 1/2]: train_loss=0.877086  [ 1584/ 5237]\n",
      "[ 1/2]: train_loss=0.937898  [ 1600/ 5237]\n",
      "[ 1/2]: train_loss=0.711946  [ 1616/ 5237]\n",
      "[ 1/2]: train_loss=0.794289  [ 1632/ 5237]\n",
      "[ 1/2]: train_loss=0.867021  [ 1648/ 5237]\n",
      "[ 1/2]: train_loss=0.796780  [ 1664/ 5237]\n",
      "[ 1/2]: train_loss=0.756886  [ 1680/ 5237]\n",
      "[ 1/2]: train_loss=0.816459  [ 1696/ 5237]\n",
      "[ 1/2]: train_loss=0.793378  [ 1712/ 5237]\n",
      "[ 1/2]: train_loss=0.675304  [ 1728/ 5237]\n",
      "[ 1/2]: train_loss=0.786264  [ 1744/ 5237]\n",
      "[ 1/2]: train_loss=0.839593  [ 1760/ 5237]\n",
      "[ 1/2]: train_loss=0.723971  [ 1776/ 5237]\n",
      "[ 1/2]: train_loss=0.740991  [ 1792/ 5237]\n",
      "[ 1/2]: train_loss=0.884986  [ 1808/ 5237]\n",
      "[ 1/2]: train_loss=0.850915  [ 1824/ 5237]\n",
      "[ 1/2]: train_loss=0.871947  [ 1840/ 5237]\n",
      "[ 1/2]: train_loss=0.687264  [ 1856/ 5237]\n",
      "[ 1/2]: train_loss=0.856622  [ 1872/ 5237]\n",
      "[ 1/2]: train_loss=0.840986  [ 1888/ 5237]\n",
      "[ 1/2]: train_loss=0.779273  [ 1904/ 5237]\n",
      "[ 1/2]: train_loss=0.652075  [ 1920/ 5237]\n",
      "[ 1/2]: train_loss=0.796266  [ 1936/ 5237]\n",
      "[ 1/2]: train_loss=0.750071  [ 1952/ 5237]\n",
      "[ 1/2]: train_loss=0.775700  [ 1968/ 5237]\n",
      "[ 1/2]: train_loss=0.747741  [ 1984/ 5237]\n",
      "[ 1/2]: train_loss=0.813845  [ 2000/ 5237]\n",
      "[ 1/2]: train_loss=0.783521  [ 2016/ 5237]\n",
      "[ 1/2]: train_loss=0.834234  [ 2032/ 5237]\n",
      "[ 1/2]: train_loss=0.798563  [ 2048/ 5237]\n",
      "[ 1/2]: train_loss=0.808060  [ 2064/ 5237]\n",
      "[ 1/2]: train_loss=0.734188  [ 2080/ 5237]\n",
      "[ 1/2]: train_loss=0.810420  [ 2096/ 5237]\n",
      "[ 1/2]: train_loss=0.799649  [ 2112/ 5237]\n",
      "[ 1/2]: train_loss=0.762443  [ 2128/ 5237]\n",
      "[ 1/2]: train_loss=0.829490  [ 2144/ 5237]\n",
      "[ 1/2]: train_loss=0.848979  [ 2160/ 5237]\n",
      "[ 1/2]: train_loss=0.877890  [ 2176/ 5237]\n",
      "[ 1/2]: train_loss=0.805327  [ 2192/ 5237]\n",
      "[ 1/2]: train_loss=0.802507  [ 2208/ 5237]\n",
      "[ 1/2]: train_loss=0.881439  [ 2224/ 5237]\n",
      "[ 1/2]: train_loss=0.813472  [ 2240/ 5237]\n",
      "[ 1/2]: train_loss=0.753238  [ 2256/ 5237]\n",
      "[ 1/2]: train_loss=0.736254  [ 2272/ 5237]\n",
      "[ 1/2]: train_loss=0.750680  [ 2288/ 5237]\n",
      "[ 1/2]: train_loss=0.822080  [ 2304/ 5237]\n",
      "[ 1/2]: train_loss=0.861466  [ 2320/ 5237]\n",
      "[ 1/2]: train_loss=0.838642  [ 2336/ 5237]\n",
      "[ 1/2]: train_loss=0.660499  [ 2352/ 5237]\n",
      "[ 1/2]: train_loss=0.953722  [ 2368/ 5237]\n",
      "[ 1/2]: train_loss=0.702625  [ 2384/ 5237]\n",
      "[ 1/2]: train_loss=0.729697  [ 2400/ 5237]\n",
      "[ 1/2]: train_loss=0.660814  [ 2416/ 5237]\n",
      "[ 1/2]: train_loss=0.854059  [ 2432/ 5237]\n",
      "[ 1/2]: train_loss=0.807670  [ 2448/ 5237]\n",
      "[ 1/2]: train_loss=0.715899  [ 2464/ 5237]\n",
      "[ 1/2]: train_loss=0.732771  [ 2480/ 5237]\n",
      "[ 1/2]: train_loss=0.870626  [ 2496/ 5237]\n",
      "[ 1/2]: train_loss=0.621468  [ 2512/ 5237]\n",
      "[ 1/2]: train_loss=0.762123  [ 2528/ 5237]\n",
      "[ 1/2]: train_loss=0.699948  [ 2544/ 5237]\n",
      "[ 1/2]: train_loss=0.846381  [ 2560/ 5237]\n",
      "[ 1/2]: train_loss=0.798765  [ 2576/ 5237]\n",
      "[ 1/2]: train_loss=0.872139  [ 2592/ 5237]\n",
      "[ 1/2]: train_loss=0.727598  [ 2608/ 5237]\n",
      "[ 1/2]: train_loss=0.848638  [ 2624/ 5237]\n",
      "[ 1/2]: train_loss=0.692746  [ 2640/ 5237]\n",
      "[ 1/2]: train_loss=0.700494  [ 2656/ 5237]\n",
      "[ 1/2]: train_loss=0.709744  [ 2672/ 5237]\n",
      "[ 1/2]: train_loss=0.734688  [ 2688/ 5237]\n",
      "[ 1/2]: train_loss=0.644516  [ 2704/ 5237]\n",
      "[ 1/2]: train_loss=0.796684  [ 2720/ 5237]\n",
      "[ 1/2]: train_loss=0.695429  [ 2736/ 5237]\n",
      "[ 1/2]: train_loss=0.801641  [ 2752/ 5237]\n",
      "[ 1/2]: train_loss=0.760323  [ 2768/ 5237]\n",
      "[ 1/2]: train_loss=0.779763  [ 2784/ 5237]\n",
      "[ 1/2]: train_loss=0.889416  [ 2800/ 5237]\n",
      "[ 1/2]: train_loss=0.763169  [ 2816/ 5237]\n",
      "[ 1/2]: train_loss=0.756726  [ 2832/ 5237]\n",
      "[ 1/2]: train_loss=0.790353  [ 2848/ 5237]\n",
      "[ 1/2]: train_loss=0.679230  [ 2864/ 5237]\n",
      "[ 1/2]: train_loss=0.880352  [ 2880/ 5237]\n",
      "[ 1/2]: train_loss=0.941212  [ 2896/ 5237]\n",
      "[ 1/2]: train_loss=0.862049  [ 2912/ 5237]\n",
      "[ 1/2]: train_loss=0.690595  [ 2928/ 5237]\n",
      "[ 1/2]: train_loss=0.761965  [ 2944/ 5237]\n",
      "[ 1/2]: train_loss=0.747287  [ 2960/ 5237]\n",
      "[ 1/2]: train_loss=0.853291  [ 2976/ 5237]\n",
      "[ 1/2]: train_loss=0.741458  [ 2992/ 5237]\n",
      "[ 1/2]: train_loss=0.726448  [ 3008/ 5237]\n",
      "[ 1/2]: train_loss=0.963158  [ 3024/ 5237]\n",
      "[ 1/2]: train_loss=0.916245  [ 3040/ 5237]\n",
      "[ 1/2]: train_loss=0.879451  [ 3056/ 5237]\n",
      "[ 1/2]: train_loss=0.798994  [ 3072/ 5237]\n",
      "[ 1/2]: train_loss=0.668872  [ 3088/ 5237]\n",
      "[ 1/2]: train_loss=0.797437  [ 3104/ 5237]\n",
      "[ 1/2]: train_loss=0.666498  [ 3120/ 5237]\n",
      "[ 1/2]: train_loss=0.720984  [ 3136/ 5237]\n",
      "[ 1/2]: train_loss=0.733031  [ 3152/ 5237]\n",
      "[ 1/2]: train_loss=0.854135  [ 3168/ 5237]\n",
      "[ 1/2]: train_loss=0.688424  [ 3184/ 5237]\n",
      "[ 1/2]: train_loss=0.661844  [ 3200/ 5237]\n",
      "[ 1/2]: train_loss=0.708598  [ 3216/ 5237]\n",
      "[ 1/2]: train_loss=0.730065  [ 3232/ 5237]\n",
      "[ 1/2]: train_loss=0.776507  [ 3248/ 5237]\n",
      "[ 1/2]: train_loss=0.796650  [ 3264/ 5237]\n",
      "[ 1/2]: train_loss=0.677725  [ 3280/ 5237]\n",
      "[ 1/2]: train_loss=0.770444  [ 3296/ 5237]\n",
      "[ 1/2]: train_loss=0.727382  [ 3312/ 5237]\n",
      "[ 1/2]: train_loss=0.738508  [ 3328/ 5237]\n",
      "[ 1/2]: train_loss=0.730719  [ 3344/ 5237]\n",
      "[ 1/2]: train_loss=0.787745  [ 3360/ 5237]\n",
      "[ 1/2]: train_loss=0.693834  [ 3376/ 5237]\n",
      "[ 1/2]: train_loss=0.786282  [ 3392/ 5237]\n",
      "[ 1/2]: train_loss=0.739465  [ 3408/ 5237]\n",
      "[ 1/2]: train_loss=0.762952  [ 3424/ 5237]\n",
      "[ 1/2]: train_loss=0.750302  [ 3440/ 5237]\n",
      "[ 1/2]: train_loss=0.784989  [ 3456/ 5237]\n",
      "[ 1/2]: train_loss=0.762275  [ 3472/ 5237]\n",
      "[ 1/2]: train_loss=0.873252  [ 3488/ 5237]\n",
      "[ 1/2]: train_loss=0.727443  [ 3504/ 5237]\n",
      "[ 1/2]: train_loss=0.848822  [ 3520/ 5237]\n",
      "[ 1/2]: train_loss=0.770095  [ 3536/ 5237]\n",
      "[ 1/2]: train_loss=0.707723  [ 3552/ 5237]\n",
      "[ 1/2]: train_loss=0.733798  [ 3568/ 5237]\n",
      "[ 1/2]: train_loss=0.641386  [ 3584/ 5237]\n",
      "[ 1/2]: train_loss=0.730061  [ 3600/ 5237]\n",
      "[ 1/2]: train_loss=0.663558  [ 3616/ 5237]\n",
      "[ 1/2]: train_loss=0.820321  [ 3632/ 5237]\n",
      "[ 1/2]: train_loss=0.686957  [ 3648/ 5237]\n",
      "[ 1/2]: train_loss=0.764567  [ 3664/ 5237]\n",
      "[ 1/2]: train_loss=0.731079  [ 3680/ 5237]\n",
      "[ 1/2]: train_loss=0.670643  [ 3696/ 5237]\n",
      "[ 1/2]: train_loss=0.719351  [ 3712/ 5237]\n",
      "[ 1/2]: train_loss=0.862267  [ 3728/ 5237]\n",
      "[ 1/2]: train_loss=0.986278  [ 3744/ 5237]\n",
      "[ 1/2]: train_loss=0.875027  [ 3760/ 5237]\n",
      "[ 1/2]: train_loss=0.879257  [ 3776/ 5237]\n",
      "[ 1/2]: train_loss=0.771436  [ 3792/ 5237]\n",
      "[ 1/2]: train_loss=0.768979  [ 3808/ 5237]\n",
      "[ 1/2]: train_loss=0.808588  [ 3824/ 5237]\n",
      "[ 1/2]: train_loss=0.713434  [ 3840/ 5237]\n",
      "[ 1/2]: train_loss=0.690377  [ 3856/ 5237]\n",
      "[ 1/2]: train_loss=0.816437  [ 3872/ 5237]\n",
      "[ 1/2]: train_loss=0.686229  [ 3888/ 5237]\n",
      "[ 1/2]: train_loss=0.729367  [ 3904/ 5237]\n",
      "[ 1/2]: train_loss=0.713415  [ 3920/ 5237]\n",
      "[ 1/2]: train_loss=0.770933  [ 3936/ 5237]\n",
      "[ 1/2]: train_loss=0.784447  [ 3952/ 5237]\n",
      "[ 1/2]: train_loss=0.823366  [ 3968/ 5237]\n",
      "[ 1/2]: train_loss=0.708247  [ 3984/ 5237]\n",
      "[ 1/2]: train_loss=0.711349  [ 4000/ 5237]\n",
      "[ 1/2]: train_loss=0.693989  [ 4016/ 5237]\n",
      "[ 1/2]: train_loss=0.764566  [ 4032/ 5237]\n",
      "[ 1/2]: train_loss=0.738252  [ 4048/ 5237]\n",
      "[ 1/2]: train_loss=0.787019  [ 4064/ 5237]\n",
      "[ 1/2]: train_loss=0.744363  [ 4080/ 5237]\n",
      "[ 1/2]: train_loss=0.786752  [ 4096/ 5237]\n",
      "[ 1/2]: train_loss=0.831821  [ 4112/ 5237]\n",
      "[ 1/2]: train_loss=0.774522  [ 4128/ 5237]\n",
      "[ 1/2]: train_loss=0.679229  [ 4144/ 5237]\n",
      "[ 1/2]: train_loss=0.706540  [ 4160/ 5237]\n",
      "[ 1/2]: train_loss=0.801362  [ 4176/ 5237]\n",
      "[ 1/2]: train_loss=0.651278  [ 4192/ 5237]\n",
      "[ 1/2]: train_loss=0.743637  [ 4208/ 5237]\n",
      "[ 1/2]: train_loss=0.787332  [ 4224/ 5237]\n",
      "[ 1/2]: train_loss=0.738258  [ 4240/ 5237]\n",
      "[ 1/2]: train_loss=0.720388  [ 4256/ 5237]\n",
      "[ 1/2]: train_loss=0.680244  [ 4272/ 5237]\n",
      "[ 1/2]: train_loss=0.788499  [ 4288/ 5237]\n",
      "[ 1/2]: train_loss=0.739482  [ 4304/ 5237]\n",
      "[ 1/2]: train_loss=0.766987  [ 4320/ 5237]\n",
      "[ 1/2]: train_loss=0.729510  [ 4336/ 5237]\n",
      "[ 1/2]: train_loss=0.631285  [ 4352/ 5237]\n",
      "[ 1/2]: train_loss=0.670439  [ 4368/ 5237]\n",
      "[ 1/2]: train_loss=0.745453  [ 4384/ 5237]\n",
      "[ 1/2]: train_loss=0.796699  [ 4400/ 5237]\n",
      "[ 1/2]: train_loss=0.722767  [ 4416/ 5237]\n",
      "[ 1/2]: train_loss=0.644620  [ 4432/ 5237]\n",
      "[ 1/2]: train_loss=0.737557  [ 4448/ 5237]\n",
      "[ 1/2]: train_loss=0.744142  [ 4464/ 5237]\n",
      "[ 1/2]: train_loss=0.854616  [ 4480/ 5237]\n",
      "[ 1/2]: train_loss=0.698105  [ 4496/ 5237]\n",
      "[ 1/2]: train_loss=0.835862  [ 4512/ 5237]\n",
      "[ 1/2]: train_loss=0.793375  [ 4528/ 5237]\n",
      "[ 1/2]: train_loss=0.784176  [ 4544/ 5237]\n",
      "[ 1/2]: train_loss=0.703041  [ 4560/ 5237]\n",
      "[ 1/2]: train_loss=0.705136  [ 4576/ 5237]\n",
      "[ 1/2]: train_loss=0.777225  [ 4592/ 5237]\n",
      "[ 1/2]: train_loss=0.740369  [ 4608/ 5237]\n",
      "[ 1/2]: train_loss=0.701461  [ 4624/ 5237]\n",
      "[ 1/2]: train_loss=0.771994  [ 4640/ 5237]\n",
      "[ 1/2]: train_loss=0.847976  [ 4656/ 5237]\n",
      "[ 1/2]: train_loss=0.709339  [ 4672/ 5237]\n",
      "[ 1/2]: train_loss=0.795660  [ 4688/ 5237]\n",
      "[ 1/2]: train_loss=0.671346  [ 4704/ 5237]\n",
      "[ 1/2]: train_loss=0.769426  [ 4720/ 5237]\n",
      "[ 1/2]: train_loss=0.740465  [ 4736/ 5237]\n",
      "[ 1/2]: train_loss=0.740791  [ 4752/ 5237]\n",
      "[ 1/2]: train_loss=0.769896  [ 4768/ 5237]\n",
      "[ 1/2]: train_loss=0.727380  [ 4784/ 5237]\n",
      "[ 1/2]: train_loss=0.802223  [ 4800/ 5237]\n",
      "[ 1/2]: train_loss=0.871658  [ 4816/ 5237]\n",
      "[ 1/2]: train_loss=0.741538  [ 4832/ 5237]\n",
      "[ 1/2]: train_loss=0.785256  [ 4848/ 5237]\n",
      "[ 1/2]: train_loss=0.639283  [ 4864/ 5237]\n",
      "[ 1/2]: train_loss=0.716832  [ 4880/ 5237]\n",
      "[ 1/2]: train_loss=0.856782  [ 4896/ 5237]\n",
      "[ 1/2]: train_loss=0.807144  [ 4912/ 5237]\n",
      "[ 1/2]: train_loss=0.761737  [ 4928/ 5237]\n",
      "[ 1/2]: train_loss=0.687037  [ 4944/ 5237]\n",
      "[ 1/2]: train_loss=0.742148  [ 4960/ 5237]\n",
      "[ 1/2]: train_loss=0.787306  [ 4976/ 5237]\n",
      "[ 1/2]: train_loss=0.787734  [ 4992/ 5237]\n",
      "[ 1/2]: train_loss=0.810050  [ 5008/ 5237]\n",
      "[ 1/2]: train_loss=0.795066  [ 5024/ 5237]\n",
      "[ 1/2]: train_loss=0.795813  [ 5040/ 5237]\n",
      "[ 1/2]: train_loss=0.759520  [ 5056/ 5237]\n",
      "[ 1/2]: train_loss=0.712174  [ 5072/ 5237]\n",
      "[ 1/2]: train_loss=0.764536  [ 5088/ 5237]\n",
      "[ 1/2]: train_loss=0.779115  [ 5104/ 5237]\n",
      "[ 1/2]: train_loss=0.914823  [ 5120/ 5237]\n",
      "[ 1/2]: train_loss=0.687472  [ 5136/ 5237]\n",
      "[ 1/2]: train_loss=0.720519  [ 5152/ 5237]\n",
      "[ 1/2]: train_loss=0.674234  [ 5168/ 5237]\n",
      "[ 1/2]: train_loss=0.733055  [ 5184/ 5237]\n",
      "[ 1/2]: train_loss=0.663643  [ 5200/ 5237]\n",
      "[ 1/2]: train_loss=0.700706  [ 5216/ 5237]\n",
      "[ 1/2]: train_loss=0.738577  [ 5232/ 5237]\n",
      "[ 1/2]: train_loss=0.733707  [ 5237/ 5237]\n",
      "[ 1/2]: train_loss=0.820147 val_loss=1.00598\n",
      "[ 2/2]: train_loss=0.679045  [   16/ 5237]\n",
      "[ 2/2]: train_loss=0.735324  [   32/ 5237]\n",
      "[ 2/2]: train_loss=0.871498  [   48/ 5237]\n",
      "[ 2/2]: train_loss=0.757107  [   64/ 5237]\n",
      "[ 2/2]: train_loss=0.719317  [   80/ 5237]\n",
      "[ 2/2]: train_loss=0.715002  [   96/ 5237]\n",
      "[ 2/2]: train_loss=0.720006  [  112/ 5237]\n",
      "[ 2/2]: train_loss=0.824611  [  128/ 5237]\n",
      "[ 2/2]: train_loss=0.733429  [  144/ 5237]\n",
      "[ 2/2]: train_loss=0.796981  [  160/ 5237]\n",
      "[ 2/2]: train_loss=0.766131  [  176/ 5237]\n",
      "[ 2/2]: train_loss=0.736106  [  192/ 5237]\n",
      "[ 2/2]: train_loss=0.810267  [  208/ 5237]\n",
      "[ 2/2]: train_loss=0.813578  [  224/ 5237]\n",
      "[ 2/2]: train_loss=0.731358  [  240/ 5237]\n",
      "[ 2/2]: train_loss=0.762177  [  256/ 5237]\n",
      "[ 2/2]: train_loss=0.690767  [  272/ 5237]\n",
      "[ 2/2]: train_loss=0.709513  [  288/ 5237]\n",
      "[ 2/2]: train_loss=0.751267  [  304/ 5237]\n",
      "[ 2/2]: train_loss=0.719702  [  320/ 5237]\n",
      "[ 2/2]: train_loss=0.665462  [  336/ 5237]\n",
      "[ 2/2]: train_loss=0.697296  [  352/ 5237]\n",
      "[ 2/2]: train_loss=0.654840  [  368/ 5237]\n",
      "[ 2/2]: train_loss=0.725792  [  384/ 5237]\n",
      "[ 2/2]: train_loss=0.737775  [  400/ 5237]\n",
      "[ 2/2]: train_loss=0.699701  [  416/ 5237]\n",
      "[ 2/2]: train_loss=0.687531  [  432/ 5237]\n",
      "[ 2/2]: train_loss=0.715432  [  448/ 5237]\n",
      "[ 2/2]: train_loss=0.725095  [  464/ 5237]\n",
      "[ 2/2]: train_loss=0.819097  [  480/ 5237]\n",
      "[ 2/2]: train_loss=0.748018  [  496/ 5237]\n",
      "[ 2/2]: train_loss=0.859835  [  512/ 5237]\n",
      "[ 2/2]: train_loss=0.693215  [  528/ 5237]\n",
      "[ 2/2]: train_loss=0.749760  [  544/ 5237]\n",
      "[ 2/2]: train_loss=0.824798  [  560/ 5237]\n",
      "[ 2/2]: train_loss=0.848715  [  576/ 5237]\n",
      "[ 2/2]: train_loss=0.710731  [  592/ 5237]\n",
      "[ 2/2]: train_loss=0.757420  [  608/ 5237]\n",
      "[ 2/2]: train_loss=0.765313  [  624/ 5237]\n",
      "[ 2/2]: train_loss=0.716774  [  640/ 5237]\n",
      "[ 2/2]: train_loss=0.750473  [  656/ 5237]\n",
      "[ 2/2]: train_loss=0.846674  [  672/ 5237]\n",
      "[ 2/2]: train_loss=0.753491  [  688/ 5237]\n",
      "[ 2/2]: train_loss=0.879819  [  704/ 5237]\n",
      "[ 2/2]: train_loss=0.854156  [  720/ 5237]\n",
      "[ 2/2]: train_loss=0.819268  [  736/ 5237]\n",
      "[ 2/2]: train_loss=0.872884  [  752/ 5237]\n",
      "[ 2/2]: train_loss=0.757691  [  768/ 5237]\n",
      "[ 2/2]: train_loss=0.839039  [  784/ 5237]\n",
      "[ 2/2]: train_loss=0.762224  [  800/ 5237]\n",
      "[ 2/2]: train_loss=0.716100  [  816/ 5237]\n",
      "[ 2/2]: train_loss=0.750620  [  832/ 5237]\n",
      "[ 2/2]: train_loss=0.845320  [  848/ 5237]\n",
      "[ 2/2]: train_loss=0.835850  [  864/ 5237]\n",
      "[ 2/2]: train_loss=0.766361  [  880/ 5237]\n",
      "[ 2/2]: train_loss=0.701286  [  896/ 5237]\n",
      "[ 2/2]: train_loss=0.727157  [  912/ 5237]\n",
      "[ 2/2]: train_loss=0.741038  [  928/ 5237]\n",
      "[ 2/2]: train_loss=0.787924  [  944/ 5237]\n",
      "[ 2/2]: train_loss=0.816345  [  960/ 5237]\n",
      "[ 2/2]: train_loss=0.734198  [  976/ 5237]\n",
      "[ 2/2]: train_loss=0.694457  [  992/ 5237]\n",
      "[ 2/2]: train_loss=0.843320  [ 1008/ 5237]\n",
      "[ 2/2]: train_loss=0.722754  [ 1024/ 5237]\n",
      "[ 2/2]: train_loss=0.681287  [ 1040/ 5237]\n",
      "[ 2/2]: train_loss=0.698864  [ 1056/ 5237]\n",
      "[ 2/2]: train_loss=0.773883  [ 1072/ 5237]\n",
      "[ 2/2]: train_loss=0.715924  [ 1088/ 5237]\n",
      "[ 2/2]: train_loss=0.882083  [ 1104/ 5237]\n",
      "[ 2/2]: train_loss=0.717479  [ 1120/ 5237]\n",
      "[ 2/2]: train_loss=0.670885  [ 1136/ 5237]\n",
      "[ 2/2]: train_loss=0.758079  [ 1152/ 5237]\n",
      "[ 2/2]: train_loss=0.726182  [ 1168/ 5237]\n",
      "[ 2/2]: train_loss=0.718248  [ 1184/ 5237]\n",
      "[ 2/2]: train_loss=0.684696  [ 1200/ 5237]\n",
      "[ 2/2]: train_loss=0.660852  [ 1216/ 5237]\n",
      "[ 2/2]: train_loss=0.601769  [ 1232/ 5237]\n",
      "[ 2/2]: train_loss=0.732744  [ 1248/ 5237]\n",
      "[ 2/2]: train_loss=0.739810  [ 1264/ 5237]\n",
      "[ 2/2]: train_loss=0.717276  [ 1280/ 5237]\n",
      "[ 2/2]: train_loss=0.604855  [ 1296/ 5237]\n",
      "[ 2/2]: train_loss=0.715349  [ 1312/ 5237]\n",
      "[ 2/2]: train_loss=0.778725  [ 1328/ 5237]\n",
      "[ 2/2]: train_loss=0.797790  [ 1344/ 5237]\n",
      "[ 2/2]: train_loss=0.667126  [ 1360/ 5237]\n",
      "[ 2/2]: train_loss=0.586202  [ 1376/ 5237]\n",
      "[ 2/2]: train_loss=0.831938  [ 1392/ 5237]\n",
      "[ 2/2]: train_loss=0.788695  [ 1408/ 5237]\n",
      "[ 2/2]: train_loss=0.782408  [ 1424/ 5237]\n",
      "[ 2/2]: train_loss=0.709645  [ 1440/ 5237]\n",
      "[ 2/2]: train_loss=0.698864  [ 1456/ 5237]\n",
      "[ 2/2]: train_loss=0.764706  [ 1472/ 5237]\n",
      "[ 2/2]: train_loss=0.716187  [ 1488/ 5237]\n",
      "[ 2/2]: train_loss=0.724054  [ 1504/ 5237]\n",
      "[ 2/2]: train_loss=0.891462  [ 1520/ 5237]\n",
      "[ 2/2]: train_loss=0.674864  [ 1536/ 5237]\n",
      "[ 2/2]: train_loss=0.665250  [ 1552/ 5237]\n",
      "[ 2/2]: train_loss=0.734509  [ 1568/ 5237]\n",
      "[ 2/2]: train_loss=0.705346  [ 1584/ 5237]\n",
      "[ 2/2]: train_loss=0.722320  [ 1600/ 5237]\n",
      "[ 2/2]: train_loss=0.660574  [ 1616/ 5237]\n",
      "[ 2/2]: train_loss=0.748873  [ 1632/ 5237]\n",
      "[ 2/2]: train_loss=0.721109  [ 1648/ 5237]\n",
      "[ 2/2]: train_loss=0.672573  [ 1664/ 5237]\n",
      "[ 2/2]: train_loss=0.766951  [ 1680/ 5237]\n",
      "[ 2/2]: train_loss=0.773352  [ 1696/ 5237]\n",
      "[ 2/2]: train_loss=0.673184  [ 1712/ 5237]\n",
      "[ 2/2]: train_loss=0.752767  [ 1728/ 5237]\n",
      "[ 2/2]: train_loss=0.768790  [ 1744/ 5237]\n",
      "[ 2/2]: train_loss=0.760271  [ 1760/ 5237]\n",
      "[ 2/2]: train_loss=0.629482  [ 1776/ 5237]\n",
      "[ 2/2]: train_loss=0.734282  [ 1792/ 5237]\n",
      "[ 2/2]: train_loss=0.741688  [ 1808/ 5237]\n",
      "[ 2/2]: train_loss=0.747823  [ 1824/ 5237]\n",
      "[ 2/2]: train_loss=0.717646  [ 1840/ 5237]\n",
      "[ 2/2]: train_loss=0.668717  [ 1856/ 5237]\n",
      "[ 2/2]: train_loss=0.701529  [ 1872/ 5237]\n",
      "[ 2/2]: train_loss=0.760544  [ 1888/ 5237]\n",
      "[ 2/2]: train_loss=0.635876  [ 1904/ 5237]\n",
      "[ 2/2]: train_loss=0.707352  [ 1920/ 5237]\n",
      "[ 2/2]: train_loss=0.767181  [ 1936/ 5237]\n",
      "[ 2/2]: train_loss=0.701102  [ 1952/ 5237]\n",
      "[ 2/2]: train_loss=0.820298  [ 1968/ 5237]\n",
      "[ 2/2]: train_loss=0.769589  [ 1984/ 5237]\n",
      "[ 2/2]: train_loss=0.665884  [ 2000/ 5237]\n",
      "[ 2/2]: train_loss=0.668786  [ 2016/ 5237]\n",
      "[ 2/2]: train_loss=0.706613  [ 2032/ 5237]\n",
      "[ 2/2]: train_loss=0.725636  [ 2048/ 5237]\n",
      "[ 2/2]: train_loss=0.735494  [ 2064/ 5237]\n",
      "[ 2/2]: train_loss=0.744745  [ 2080/ 5237]\n",
      "[ 2/2]: train_loss=0.634560  [ 2096/ 5237]\n",
      "[ 2/2]: train_loss=0.755454  [ 2112/ 5237]\n",
      "[ 2/2]: train_loss=0.892728  [ 2128/ 5237]\n",
      "[ 2/2]: train_loss=0.734707  [ 2144/ 5237]\n",
      "[ 2/2]: train_loss=0.762574  [ 2160/ 5237]\n",
      "[ 2/2]: train_loss=0.654887  [ 2176/ 5237]\n",
      "[ 2/2]: train_loss=0.708330  [ 2192/ 5237]\n",
      "[ 2/2]: train_loss=0.769198  [ 2208/ 5237]\n",
      "[ 2/2]: train_loss=0.791890  [ 2224/ 5237]\n",
      "[ 2/2]: train_loss=0.707711  [ 2240/ 5237]\n",
      "[ 2/2]: train_loss=0.683660  [ 2256/ 5237]\n",
      "[ 2/2]: train_loss=0.713203  [ 2272/ 5237]\n",
      "[ 2/2]: train_loss=0.744683  [ 2288/ 5237]\n",
      "[ 2/2]: train_loss=0.768355  [ 2304/ 5237]\n",
      "[ 2/2]: train_loss=0.759380  [ 2320/ 5237]\n",
      "[ 2/2]: train_loss=0.751960  [ 2336/ 5237]\n",
      "[ 2/2]: train_loss=0.691051  [ 2352/ 5237]\n",
      "[ 2/2]: train_loss=0.723938  [ 2368/ 5237]\n",
      "[ 2/2]: train_loss=0.769481  [ 2384/ 5237]\n",
      "[ 2/2]: train_loss=0.815928  [ 2400/ 5237]\n",
      "[ 2/2]: train_loss=0.691688  [ 2416/ 5237]\n",
      "[ 2/2]: train_loss=0.801288  [ 2432/ 5237]\n",
      "[ 2/2]: train_loss=0.735887  [ 2448/ 5237]\n",
      "[ 2/2]: train_loss=0.655456  [ 2464/ 5237]\n",
      "[ 2/2]: train_loss=0.730285  [ 2480/ 5237]\n",
      "[ 2/2]: train_loss=0.713153  [ 2496/ 5237]\n",
      "[ 2/2]: train_loss=0.659850  [ 2512/ 5237]\n",
      "[ 2/2]: train_loss=0.733923  [ 2528/ 5237]\n",
      "[ 2/2]: train_loss=0.637164  [ 2544/ 5237]\n",
      "[ 2/2]: train_loss=0.620834  [ 2560/ 5237]\n",
      "[ 2/2]: train_loss=0.741362  [ 2576/ 5237]\n",
      "[ 2/2]: train_loss=0.727520  [ 2592/ 5237]\n",
      "[ 2/2]: train_loss=0.779646  [ 2608/ 5237]\n",
      "[ 2/2]: train_loss=0.763362  [ 2624/ 5237]\n",
      "[ 2/2]: train_loss=0.759116  [ 2640/ 5237]\n",
      "[ 2/2]: train_loss=0.755889  [ 2656/ 5237]\n",
      "[ 2/2]: train_loss=0.717668  [ 2672/ 5237]\n",
      "[ 2/2]: train_loss=0.717871  [ 2688/ 5237]\n",
      "[ 2/2]: train_loss=0.731528  [ 2704/ 5237]\n",
      "[ 2/2]: train_loss=0.794523  [ 2720/ 5237]\n",
      "[ 2/2]: train_loss=0.806358  [ 2736/ 5237]\n",
      "[ 2/2]: train_loss=0.645223  [ 2752/ 5237]\n",
      "[ 2/2]: train_loss=0.739166  [ 2768/ 5237]\n",
      "[ 2/2]: train_loss=0.781529  [ 2784/ 5237]\n",
      "[ 2/2]: train_loss=0.623533  [ 2800/ 5237]\n",
      "[ 2/2]: train_loss=0.695716  [ 2816/ 5237]\n",
      "[ 2/2]: train_loss=0.642429  [ 2832/ 5237]\n",
      "[ 2/2]: train_loss=0.778370  [ 2848/ 5237]\n",
      "[ 2/2]: train_loss=0.720173  [ 2864/ 5237]\n",
      "[ 2/2]: train_loss=0.730967  [ 2880/ 5237]\n",
      "[ 2/2]: train_loss=0.702446  [ 2896/ 5237]\n",
      "[ 2/2]: train_loss=0.877441  [ 2912/ 5237]\n",
      "[ 2/2]: train_loss=0.621526  [ 2928/ 5237]\n",
      "[ 2/2]: train_loss=0.688861  [ 2944/ 5237]\n",
      "[ 2/2]: train_loss=0.765520  [ 2960/ 5237]\n",
      "[ 2/2]: train_loss=0.684897  [ 2976/ 5237]\n",
      "[ 2/2]: train_loss=0.689086  [ 2992/ 5237]\n",
      "[ 2/2]: train_loss=0.807320  [ 3008/ 5237]\n",
      "[ 2/2]: train_loss=0.698684  [ 3024/ 5237]\n",
      "[ 2/2]: train_loss=0.662179  [ 3040/ 5237]\n",
      "[ 2/2]: train_loss=0.760500  [ 3056/ 5237]\n",
      "[ 2/2]: train_loss=0.797069  [ 3072/ 5237]\n",
      "[ 2/2]: train_loss=0.726058  [ 3088/ 5237]\n",
      "[ 2/2]: train_loss=0.788487  [ 3104/ 5237]\n",
      "[ 2/2]: train_loss=0.734578  [ 3120/ 5237]\n",
      "[ 2/2]: train_loss=0.774793  [ 3136/ 5237]\n",
      "[ 2/2]: train_loss=0.717940  [ 3152/ 5237]\n",
      "[ 2/2]: train_loss=0.671223  [ 3168/ 5237]\n",
      "[ 2/2]: train_loss=0.810675  [ 3184/ 5237]\n",
      "[ 2/2]: train_loss=0.684292  [ 3200/ 5237]\n",
      "[ 2/2]: train_loss=0.751558  [ 3216/ 5237]\n",
      "[ 2/2]: train_loss=0.673509  [ 3232/ 5237]\n",
      "[ 2/2]: train_loss=0.823198  [ 3248/ 5237]\n",
      "[ 2/2]: train_loss=0.653220  [ 3264/ 5237]\n",
      "[ 2/2]: train_loss=0.844227  [ 3280/ 5237]\n",
      "[ 2/2]: train_loss=0.774558  [ 3296/ 5237]\n",
      "[ 2/2]: train_loss=0.770909  [ 3312/ 5237]\n",
      "[ 2/2]: train_loss=0.640526  [ 3328/ 5237]\n",
      "[ 2/2]: train_loss=0.733218  [ 3344/ 5237]\n",
      "[ 2/2]: train_loss=0.687709  [ 3360/ 5237]\n",
      "[ 2/2]: train_loss=0.688589  [ 3376/ 5237]\n",
      "[ 2/2]: train_loss=0.749041  [ 3392/ 5237]\n",
      "[ 2/2]: train_loss=0.738180  [ 3408/ 5237]\n",
      "[ 2/2]: train_loss=0.814851  [ 3424/ 5237]\n",
      "[ 2/2]: train_loss=0.813007  [ 3440/ 5237]\n",
      "[ 2/2]: train_loss=0.669046  [ 3456/ 5237]\n",
      "[ 2/2]: train_loss=0.806138  [ 3472/ 5237]\n",
      "[ 2/2]: train_loss=0.690956  [ 3488/ 5237]\n",
      "[ 2/2]: train_loss=0.681815  [ 3504/ 5237]\n",
      "[ 2/2]: train_loss=0.720838  [ 3520/ 5237]\n",
      "[ 2/2]: train_loss=0.796742  [ 3536/ 5237]\n",
      "[ 2/2]: train_loss=0.676383  [ 3552/ 5237]\n",
      "[ 2/2]: train_loss=0.775968  [ 3568/ 5237]\n",
      "[ 2/2]: train_loss=0.836277  [ 3584/ 5237]\n",
      "[ 2/2]: train_loss=0.828201  [ 3600/ 5237]\n",
      "[ 2/2]: train_loss=0.710784  [ 3616/ 5237]\n",
      "[ 2/2]: train_loss=0.680876  [ 3632/ 5237]\n",
      "[ 2/2]: train_loss=0.778485  [ 3648/ 5237]\n",
      "[ 2/2]: train_loss=0.735708  [ 3664/ 5237]\n",
      "[ 2/2]: train_loss=0.787639  [ 3680/ 5237]\n",
      "[ 2/2]: train_loss=0.754578  [ 3696/ 5237]\n",
      "[ 2/2]: train_loss=0.743735  [ 3712/ 5237]\n",
      "[ 2/2]: train_loss=0.716532  [ 3728/ 5237]\n",
      "[ 2/2]: train_loss=0.691284  [ 3744/ 5237]\n",
      "[ 2/2]: train_loss=0.687583  [ 3760/ 5237]\n",
      "[ 2/2]: train_loss=0.626488  [ 3776/ 5237]\n",
      "[ 2/2]: train_loss=0.692953  [ 3792/ 5237]\n",
      "[ 2/2]: train_loss=0.690484  [ 3808/ 5237]\n",
      "[ 2/2]: train_loss=0.713207  [ 3824/ 5237]\n",
      "[ 2/2]: train_loss=0.686932  [ 3840/ 5237]\n",
      "[ 2/2]: train_loss=0.657052  [ 3856/ 5237]\n",
      "[ 2/2]: train_loss=0.840368  [ 3872/ 5237]\n",
      "[ 2/2]: train_loss=0.746040  [ 3888/ 5237]\n",
      "[ 2/2]: train_loss=0.698487  [ 3904/ 5237]\n",
      "[ 2/2]: train_loss=0.695083  [ 3920/ 5237]\n",
      "[ 2/2]: train_loss=0.653998  [ 3936/ 5237]\n",
      "[ 2/2]: train_loss=0.822852  [ 3952/ 5237]\n",
      "[ 2/2]: train_loss=0.879170  [ 3968/ 5237]\n",
      "[ 2/2]: train_loss=0.811848  [ 3984/ 5237]\n",
      "[ 2/2]: train_loss=0.795830  [ 4000/ 5237]\n",
      "[ 2/2]: train_loss=0.678703  [ 4016/ 5237]\n",
      "[ 2/2]: train_loss=0.799222  [ 4032/ 5237]\n",
      "[ 2/2]: train_loss=0.645824  [ 4048/ 5237]\n",
      "[ 2/2]: train_loss=0.753547  [ 4064/ 5237]\n",
      "[ 2/2]: train_loss=0.709382  [ 4080/ 5237]\n",
      "[ 2/2]: train_loss=0.674527  [ 4096/ 5237]\n",
      "[ 2/2]: train_loss=0.696764  [ 4112/ 5237]\n",
      "[ 2/2]: train_loss=0.777412  [ 4128/ 5237]\n",
      "[ 2/2]: train_loss=0.734681  [ 4144/ 5237]\n",
      "[ 2/2]: train_loss=0.703081  [ 4160/ 5237]\n",
      "[ 2/2]: train_loss=0.649517  [ 4176/ 5237]\n",
      "[ 2/2]: train_loss=0.766370  [ 4192/ 5237]\n",
      "[ 2/2]: train_loss=0.840038  [ 4208/ 5237]\n",
      "[ 2/2]: train_loss=0.682382  [ 4224/ 5237]\n",
      "[ 2/2]: train_loss=0.806701  [ 4240/ 5237]\n",
      "[ 2/2]: train_loss=0.686286  [ 4256/ 5237]\n",
      "[ 2/2]: train_loss=0.636942  [ 4272/ 5237]\n",
      "[ 2/2]: train_loss=0.751975  [ 4288/ 5237]\n",
      "[ 2/2]: train_loss=0.809899  [ 4304/ 5237]\n",
      "[ 2/2]: train_loss=0.638673  [ 4320/ 5237]\n",
      "[ 2/2]: train_loss=0.735119  [ 4336/ 5237]\n",
      "[ 2/2]: train_loss=0.775467  [ 4352/ 5237]\n",
      "[ 2/2]: train_loss=0.734379  [ 4368/ 5237]\n",
      "[ 2/2]: train_loss=0.733358  [ 4384/ 5237]\n",
      "[ 2/2]: train_loss=0.814337  [ 4400/ 5237]\n",
      "[ 2/2]: train_loss=0.694135  [ 4416/ 5237]\n",
      "[ 2/2]: train_loss=0.720722  [ 4432/ 5237]\n",
      "[ 2/2]: train_loss=0.609263  [ 4448/ 5237]\n",
      "[ 2/2]: train_loss=0.752962  [ 4464/ 5237]\n",
      "[ 2/2]: train_loss=0.741265  [ 4480/ 5237]\n",
      "[ 2/2]: train_loss=0.737198  [ 4496/ 5237]\n",
      "[ 2/2]: train_loss=0.672427  [ 4512/ 5237]\n",
      "[ 2/2]: train_loss=0.781993  [ 4528/ 5237]\n",
      "[ 2/2]: train_loss=0.772500  [ 4544/ 5237]\n",
      "[ 2/2]: train_loss=0.761175  [ 4560/ 5237]\n",
      "[ 2/2]: train_loss=0.784165  [ 4576/ 5237]\n",
      "[ 2/2]: train_loss=0.799668  [ 4592/ 5237]\n",
      "[ 2/2]: train_loss=0.703179  [ 4608/ 5237]\n",
      "[ 2/2]: train_loss=0.818922  [ 4624/ 5237]\n",
      "[ 2/2]: train_loss=0.755582  [ 4640/ 5237]\n",
      "[ 2/2]: train_loss=0.625874  [ 4656/ 5237]\n",
      "[ 2/2]: train_loss=0.686263  [ 4672/ 5237]\n",
      "[ 2/2]: train_loss=0.856429  [ 4688/ 5237]\n",
      "[ 2/2]: train_loss=0.728986  [ 4704/ 5237]\n",
      "[ 2/2]: train_loss=0.714022  [ 4720/ 5237]\n",
      "[ 2/2]: train_loss=0.720998  [ 4736/ 5237]\n",
      "[ 2/2]: train_loss=0.769846  [ 4752/ 5237]\n",
      "[ 2/2]: train_loss=0.733489  [ 4768/ 5237]\n",
      "[ 2/2]: train_loss=0.678176  [ 4784/ 5237]\n",
      "[ 2/2]: train_loss=0.658844  [ 4800/ 5237]\n",
      "[ 2/2]: train_loss=0.617164  [ 4816/ 5237]\n",
      "[ 2/2]: train_loss=0.743510  [ 4832/ 5237]\n",
      "[ 2/2]: train_loss=0.735479  [ 4848/ 5237]\n",
      "[ 2/2]: train_loss=0.776432  [ 4864/ 5237]\n",
      "[ 2/2]: train_loss=0.575497  [ 4880/ 5237]\n",
      "[ 2/2]: train_loss=0.707125  [ 4896/ 5237]\n",
      "[ 2/2]: train_loss=0.721268  [ 4912/ 5237]\n",
      "[ 2/2]: train_loss=0.665343  [ 4928/ 5237]\n",
      "[ 2/2]: train_loss=0.647154  [ 4944/ 5237]\n",
      "[ 2/2]: train_loss=0.716721  [ 4960/ 5237]\n",
      "[ 2/2]: train_loss=0.653637  [ 4976/ 5237]\n",
      "[ 2/2]: train_loss=0.714339  [ 4992/ 5237]\n",
      "[ 2/2]: train_loss=0.766597  [ 5008/ 5237]\n",
      "[ 2/2]: train_loss=0.774136  [ 5024/ 5237]\n",
      "[ 2/2]: train_loss=0.810207  [ 5040/ 5237]\n",
      "[ 2/2]: train_loss=0.858111  [ 5056/ 5237]\n",
      "[ 2/2]: train_loss=0.764895  [ 5072/ 5237]\n",
      "[ 2/2]: train_loss=0.766828  [ 5088/ 5237]\n",
      "[ 2/2]: train_loss=0.635902  [ 5104/ 5237]\n",
      "[ 2/2]: train_loss=0.715709  [ 5120/ 5237]\n",
      "[ 2/2]: train_loss=0.715855  [ 5136/ 5237]\n",
      "[ 2/2]: train_loss=0.658478  [ 5152/ 5237]\n",
      "[ 2/2]: train_loss=0.731559  [ 5168/ 5237]\n",
      "[ 2/2]: train_loss=0.707572  [ 5184/ 5237]\n",
      "[ 2/2]: train_loss=0.758616  [ 5200/ 5237]\n",
      "[ 2/2]: train_loss=0.816111  [ 5216/ 5237]\n",
      "[ 2/2]: train_loss=0.686050  [ 5232/ 5237]\n",
      "[ 2/2]: train_loss=0.665131  [ 5237/ 5237]\n",
      "[ 2/2]: train_loss=0.734868 val_loss=1.04957\n",
      "\n",
      "best model:\n",
      "\tval_loss=1.005981\n"
     ]
    }
   ],
   "source": [
    "class_weights=[1.0 for _ in range(8)]\n",
    "loss_weight_dict = {'loss_labels': 0.33, 'loss_bbox': 0.33, 'loss_giou': 0.33}\n",
    "loss_fn = HungarianLoss(num_classes=len(class_names), class_weights=class_weights, empty_weight=0.2, loss_weight_dict=loss_weight_dict)\n",
    "\n",
    "train_model(\n",
    "    model,\n",
    "    device='cuda:0',\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.001),\n",
    "    epochs=2,\n",
    "    early_stopping_patience=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
